ğŸš€ CrawlerZero: Advanced Data Integration System

CrawlerZero, web sitelerinden hiyerarÅŸik olarak veri toplayan, dÃ¶kÃ¼manlarÄ± (PDF, DOCX, vb.) ayÄ±klayan ve bunlarÄ± hem yerel dosya sisteminde hem de PostgreSQL Ã¼zerinde indeksleyen yÃ¼ksek performanslÄ± bir asenkron tarayÄ±cÄ± sistemidir.
âœ¨ Ã–ne Ã‡Ä±kan Ã–zellikler

    âš¡ Asenkron Mimari: asyncio ve asyncpg ile aynÄ± anda onlarca sayfayÄ± ve dosyayÄ± tarayabilir.

    ğŸ“‚ AkÄ±llÄ± DÃ¶kÃ¼man Ä°ÅŸleme: PDF, DOC, DOCX, PPTX ve XLSX dosyalarÄ±ndan otomatik metin Ã§Ä±karma.

    ğŸ¯ Path Mode: Sadece belirli bir alt dizin (path) altÄ±ndaki iÃ§eriÄŸe odaklanma.

    ğŸ“„ Documents Only Modu: HTML sayfalarÄ±nÄ± sadece link bulmak iÃ§in kullanÄ±p, veritabanÄ±nÄ± sadece deÄŸerli dÃ¶kÃ¼manlarla doldurma.

    ğŸ”„ Incremental Crawling: content_hash kontrolÃ¼ ile sadece deÄŸiÅŸen veya yeni eklenen iÃ§erikleri iÅŸleme.

    ğŸ›¡ï¸ Hata ToleransÄ±: Bozuk dosyalar veya karakter seti (encoding) sorunlarÄ±na karÅŸÄ± dayanÄ±klÄ± yapÄ±.

ğŸ› ï¸ Kurulum
1. Gereksinimler

    Python 3.10+

    PostgreSQL 14+

    Virtualenv (Ã–nerilir)

2. BaÄŸÄ±mlÄ±lÄ±klarÄ±n YÃ¼klenmesi
Bash

git clone https://github.com/kullanici/crawler_zero.git
cd crawler_zero
pip install -r requirements.sh

3. VeritabanÄ± HazÄ±rlÄ±ÄŸÄ±

PostgreSQL Ã¼zerinde aÅŸaÄŸÄ±daki tablolarÄ± oluÅŸturun:
SQL

-- Ä°ÅŸ Takip Tablosu
CREATE TABLE jobs (
    job_id UUID PRIMARY KEY,
    start_url TEXT NOT NULL,
    config JSONB,
    documents_only BOOLEAN DEFAULT FALSE,
    status TEXT DEFAULT 'PENDING',
    agent_id TEXT,
    project_id INT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Veri Tablosu
CREATE TABLE raw_documents (
    id SERIAL PRIMARY KEY,
    source_type TEXT, -- 'page' veya 'file'
    source_id TEXT,
    url TEXT,
    site TEXT,
    raw_text TEXT,
    content_hash TEXT UNIQUE,
    content_type TEXT,
    text_len INT,
    agent_id TEXT,
    project_id INT,
    created_at TIMESTAMP DEFAULT NOW()
);

ğŸš€ Ã‡alÄ±ÅŸtÄ±rma

Sistemi tek bir komutla ayaÄŸa kaldÄ±rmak iÃ§in launcher.py scriptini kullanabilirsiniz:
Bash

python3 launcher.py

Bu komut hem FastAPI sunucusunu hem de Worker Daemon'Ä± paralel olarak baÅŸlatÄ±r.
ğŸ“‘ API KullanÄ±mÄ±
Yeni Bir Tarama BaÅŸlatma

POST /jobs endpoint'ine bir JSON gÃ¶ndererek iÅŸlemi baÅŸlatÄ±n:
Bash

curl -X POST http://127.0.0.1:8000/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.python.org/3/tutorial/",
    "path_mode": true,
    "documents_only": true,
    "concurrency": 8,
    "agent_id": "agent_007",
    "project_id": 1
  }'

Parametre AÃ§Ä±klamalarÄ±
Parametre	AÃ§Ä±klama
path_mode	true ise sadece baÅŸlangÄ±Ã§ URL'inin alt klasÃ¶rlerini tarar.
documents_only	true ise HTML metinlerini DB'ye kaydetmez, sadece dosyalarÄ± (PDF vb.) kaydeder.
download_only_same_domain	DÄ±ÅŸ sitelere verilen dÃ¶kÃ¼man linklerini indirmeyi engeller.
incremental	Daha Ã¶nce Ã§ekilen ve deÄŸiÅŸmeyen iÃ§erikleri atlar.
ğŸ“‚ Dosya YapÄ±sÄ±

    api/: FastAPI endpointleri ve istek modelleri.

    workers/: Arka planda Ã§alÄ±ÅŸan tarayÄ±cÄ± (crawler) mantÄ±ÄŸÄ±.

    storage/: Yerel dosya sistemi (JSON index) yÃ¶netimi.

    db/: PostgreSQL baÄŸlantÄ± ve sorgu katmanÄ±.

    utils/: Hashleme, domain ayÄ±klama ve metin temizleme araÃ§larÄ±.

ğŸ“ Lisans

Bu proje MIT lisansÄ± altÄ±nda korunmaktadÄ±r.
